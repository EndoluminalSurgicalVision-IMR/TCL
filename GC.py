from __future__ import absolute_import, division, print_function

import time
import numpy as np
import torch
from utils import mono_GC,warp_depth_with_pose,disp_to_depth
from torchvision import transforms
from PIL import Image  # using pillow-simd for increased speed
from torch import nn
import torch.nn.functional as F

L1Loss = nn.L1Loss()

class Trainer:
    """
    We utilize the code architecture of AF-SfMlearner/Monodepth2.
    You can embed the following module into your code based on the specific SfM baseline structure and dataset.
    Please add additional basic functions according to your specific requirements.
    We present the code directly related to the GC module.
    """
    def __init__(self, options):
        """
        Please follow AF-SfMlearner/Monodepth2 structure to input the options
            and define them in the options.py file.
        Define your own network architecture, dataloader, and model parameters:
        """
        self.opt = options

    def train(self):
        """Run the entire training pipeline

        """
        self.epoch = 0
        self.step = 0
        self.start_time = time.time()
        self.val = 0
        for self.epoch in range(self.opt.num_epochs):
        ## !!! Note :Please validate the entire validation set after each epoch, rather than validating after each batch.
            self.train_epoch()
            self.val_epoch()

        ### other process(log,save model......)


    def train_epoch(self):
        """Run a single epoch of training and validation
        """
        self.w_ldc = self.opt.depth_wl_weight
        self.w_lpc = self.opt.pose_wl_weight

        for batch_idx, inputs in enumerate(self.train_loader):

            self.seed = 10010 + self.step
            torch.manual_seed(self.seed)
            np.random.seed(self.seed)
            # The self.choice parameter is used to determine whether to use the raw triplet
            #       or synthesis triplet at the current batch to calculate photometric loss.
            self.choice = np.random.randint(0, 2, 1)

            inputs = self.GC_sample(inputs)

            self.set_train()
            outputs, losses = self.process_batch(inputs)


    def val_epoch(self):
        """Run a single epoch of validation
           !!! Note :Please validate the entire validation set after each epoch, rather than validating after each batch.
        """


    def GC_sample(self, inputs):

        """
        Generate synthesis triplet and valid mask.

        :param Inputs: a dictionary structure. Please construct it following the architecture of AF-SfMlearner/Monodepth2.
               Additional parameters required in input dict include:
                    "index": the index of the current target frame in the entire training set.
                    "do_flip": whether data augmentation with flipping is performed in the dataloader.
                    "pre_dgt": pre-stored depth prediction map corresponding to the current triplet.

        :return: inputs with synthesis triplet and valid mask
        """
        if self.w_lpc or self.w_ldc or self.choice:
            inputs,mask_GC_scared = self.process_synthsis_triplet(inputs)


        if self.w_ldc:
            inputs['valid_mask_tgt'] = mask_GC_scared
        if not self.choice:
            inputs['valid_mask'] =  torch.ones_like(inputs[("color", 0, 0)][:,:1,:,:]) > 0
        elif self.choice:
            inputs['valid_mask'] = mask_GC_scared

        return inputs

    def process_synthsis_triplet(self,inputs):
        index = inputs['index']

        inputs_temp=mono_GC(index, inputs, self.opt, self.opt.bound)
        inputs["pose_GC"] = inputs_temp["pose_GC"]
        ## Mask the invalid regions generated by perspective transformations.
        mask_GC = ((inputs_temp[("pre_dgt_GC", 0, 0)][:, 0, :, :] > 0) & (inputs_temp[("pre_dgt_GC", 0, 0)][:, 0, :, :] > 0)).unsqueeze(1)
        #
        for f_i in self.opt.frame_ids:
            inputs_temp[("color_GC", f_i, 0)]= inputs_temp[("color_GC", f_i, 0)] * mask_GC
            inputs_temp[("color_aug_GC", f_i, 0)]= inputs_temp[("color_aug_GC", f_i, 0)] * mask_GC

        mask_GC_scared = mask_GC

        if self.w_lpc or self.w_ldc:
            for f_i in self.opt.frame_ids:
                inputs[("color_full", f_i, 0)] = torch.cat(
                    (inputs[("color", f_i, 0)], inputs_temp[("color_GC", f_i, 0)]), 0)
                inputs[("color_aug_full", f_i, 0)] = torch.cat(
                    (inputs[("color_aug", f_i, 0)], inputs_temp[("color_aug_GC", f_i, 0)]), 0)


        if self.choice:
            self.height = 256
            self.width = 320
            self.interp = Image.ANTIALIAS
            self.resize = {}
            for i in range(self.num_scales):
                s = 2 ** i
                self.resize[i] = transforms.Resize((self.height // s, self.width // s),
                                                   interpolation=self.interp)

            for f_i in self.opt.frame_ids:

                inputs[("color", f_i, 0)] = inputs_temp[("color_GC", f_i, 0)]
                inputs[("color_aug", f_i, 0)]=inputs_temp[("color_aug_GC", f_i, 0)]
                # inputs[("pre_dgt", f_i, 0)] = inputs_temp[("pre_dgt_GC", f_i, 0)]

            for i in range(self.num_scales):
                if i>0:
                    B=inputs[("color", 0, 0)].shape[0]
                    t_to_resize=inputs[("color", 0, i - 1)]
                    t_resized_list=[]
                    for j in range(B):
                        d = transforms.ToPILImage()(t_to_resize[j])
                        img_pil=self.resize[i](d)
                        tensor_resized=transforms.ToTensor()(img_pil)
                        t_resized_list.append(tensor_resized.unsqueeze(0))
                    inputs[("color", 0, i)] = torch.cat(t_resized_list)


        return inputs,mask_GC_scared


    def process_batch(self, inputs):
        """Pass a minibatch through the network and generate images and losses

        """
        outputs ={}
        ####
        # Other basic functions

        # In endoscopic scenarios, the endoscope may move towards any direction.
        # Please utilize the pose estimation function from AF-SfMlearner: self.predict_poses_AF
        ####


        losses = self.compute_losses(inputs, outputs)

        return outputs, losses


    def compute_losses(self, inputs, outputs):

        losses = {}
        total_loss = 0

        total_repro_loss = 0

        for scale in self.opt.scales:

            loss = 0
            loss_reprojection = 0


            if self.opt.v1_multiscale:
                source_scale = scale
            else:
                source_scale = 0

            disp = outputs[("disp", scale)]
            color = inputs[("color", 0, scale)]
            target = inputs[("color", 0, source_scale)]


            batch, _, height, width =inputs[("color", 0, 0)].size()

            ## The "mask_ref_illu" is reserved for triplet masking in the AiC module.
            ##  If the AiC module is not included, this matrix can be set as an all-one matrix.
            outputs[("mask_ref_illu", -1)] = (torch.ones(batch, 1, height, width)+0.0).detach().cuda()
            outputs[("mask_ref_illu", 1)] = (torch.ones(batch, 1, height, width)+0.0).detach().cuda()

            for frame_id in self.opt.frame_ids[1:]:
                pred = outputs[("color", frame_id, scale)]
                reprojection_losses = self.compute_reprojection_loss(pred, target) * outputs[
                    ("mask_ref_illu", frame_id)]
                reprojection_losses = reprojection_losses * inputs['valid_mask'] * outputs[
                    ("valid_mask", frame_id, scale)]
                # reprojection_losses.append(reprojection_losses)
                loss_reprojection += reprojection_losses.mean()

        if self.w_ldc:
            ### depth consistency loss
            if self.epoch>= self.opt.dc_warm:
                # # use multiscale ldc method
                loss_4=0

                features_d = self.models["encoder"](inputs["color_aug_full", 0, 0])
                outputs_ldc = self.models["depth"](features_d)

                for scale in self.opt.scales:
                    tgt_disp_full_raw = outputs_ldc[("disp", scale)]
                    tgt_disp_full = F.interpolate(
                        tgt_disp_full_raw, [self.opt.height, self.opt.width], mode="bilinear",
                        align_corners=False)
                    _, tgt_depth_full = disp_to_depth(tgt_disp_full, self.opt.min_depth, self.opt.max_depth)

                    pose_perturbed = inputs["pose_GC"].detach().cuda()
                    K = inputs[("K", 0)][:, :-1, :-1]
                    K_inv = inputs[("inv_K", 0)][:, :-1, :-1]
                    warped_old_tgt_depth, new_tgt_depth = warp_depth_with_pose(tgt_depth_full, pose_perturbed,
                                                                               K,
                                                                               K_inv)
                    aug_mask = 1 - inputs["do_flip"]

                    loss_4_scale = self.compute_depth_l1_loss(warped_old_tgt_depth, new_tgt_depth, self.opt,
                                                              aug_mask,
                                                              inputs['valid_mask_tgt'].clone())

                    loss_4+=loss_4_scale
                loss_4/= self.num_scales

            else:
                loss_4 = torch.tensor(0)

        else:
            loss_4 = torch.tensor(0)

        if self.w_lpc :
            ### pose consistency loss
            if self.epoch >= self.opt.pc_warm:
                features_p = self.models["encoder"](inputs["color_aug_full", 0, 0])

                outputs_p=self.predict_poses(inputs, features_p)

                aug_mask = 1 - inputs["do_flip"]

                loss_5 = self.compute_pose_warp_loss(outputs_p, aug_mask, self.opt.w_trans)
            else:
                loss_5 = torch.tensor(0)
        else:
            loss_5 = torch.tensor(0)

        total_loss /= self.num_scales
        total_loss += self.w_ldc * loss_4 + self.w_lpc * loss_5

    def compute_depth_l1_loss(self,depth0, depth1, args, aug_mask=None, mask=None):

        mask = mask.detach()

        if mask is not None:
            for i in range(mask.shape[0]):
                mask[i] = (mask[i] + 0) * aug_mask[i]

            depth_0_valid = depth0[mask]
            depth_1_valid = depth1[mask]
            if len(depth_0_valid)*len(depth_1_valid)==0 :
                return torch.tensor(0.0)

            loss = L1Loss(depth_0_valid, depth_1_valid)
        else:
            loss = L1Loss(depth0, depth1)
        return loss

    def compute_pose_warp_loss(self,outputs, aug_mask, t_weight):
        B = int(outputs[('cam_T_cam', 0, -1)].shape[0] / 2)

        aug_mask = aug_mask + 0.0
        mat_0=outputs[('cam_T_cam', 0, -1)]
        mat_1=outputs[('cam_T_cam', 0, 1)]
        for i in range(B):
            mat_0[i, :] *= aug_mask[i]
            mat_0[i + B, :] *= aug_mask[i]
            mat_1[i, :] *= aug_mask[i]
            mat_1[i + B, :] *= aug_mask[i]

        mat_0_0 = mat_0[0:B, :]
        mat_0_1 = mat_0[B:, :]

        R_0_0, R_0_1 = mat_0_0[:, :3, :3], mat_0_1[:, :3, :3]
        T_0_0, T_0_1 = mat_0_0[:, :3, 3:], mat_0_1[:, :3, 3:]

        mat_1_0 = mat_1[0:B, :]
        mat_1_1 = mat_1[B:, :]
        R_1_0, R_1_1 = mat_1_0[:, :3, :3], mat_1_1[:, :3, :3]
        T_1_0, T_1_1 = mat_1_0[:, :3, 3:], mat_1_1[:, :3, 3:]

        T_all_0 = torch.cat((T_0_0, T_1_0), dim=0)
        T_all_1 = torch.cat((T_0_1, T_1_1), dim=0)

        R_all_0=torch.cat((R_0_0,R_1_0),dim=0)
        R_all_1=torch.cat((R_0_1,R_1_1),dim=0)

        loss_t=L1Loss(T_all_0, T_all_1)

        eps = 1e-6
        product = torch.bmm(R_all_0, R_all_1.transpose(1, 2))
        trace = torch.sum(product[:, torch.eye(3).bool()], 1)
        theta = torch.clamp(0.5 * (trace - 1), -1 + eps, 1 - eps)
        loss_r = torch.acos(theta).sum()

        loss = loss_r + t_weight * loss_t

        return loss


    def predict_poses_AF(self, inputs, features):
        """
            Modified from AF-SfMlearner.
            Predict poses between input frames for monocular sequences.
        """
        outputs = {}
        if self.num_pose_frames == 2:
            if self.opt.pose_model_type == "shared":
                pose_feats = {f_i: features[f_i] for f_i in self.opt.frame_ids}
            else:
                pose_feats = {f_i: inputs["color_aug", f_i, 0] for f_i in self.opt.frame_ids}

            for f_i in self.opt.frame_ids[1:]:

                if f_i != "s":

                    inputs_all = [pose_feats[f_i], pose_feats[0]]
                    inputs_all_reverse = [pose_feats[0], pose_feats[f_i]]

                    # pose
                    pose_inputs = [self.models["pose_encoder"](torch.cat(inputs_all, 1))]
                    axisangle, translation = self.models["pose"](pose_inputs)

                    outputs[("axisangle", 0, f_i)] = axisangle
                    outputs[("translation", 0, f_i)] = translation
                    outputs[("cam_T_cam", 0, f_i)] = transformation_from_parameters(
                        axisangle[:, 0], translation[:, 0])

        return outputs